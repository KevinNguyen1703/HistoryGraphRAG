import torch
import openai
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from graphrag.custom_llm_call import call_openai, ollama_complete
import ast# ğŸ”¹ Configuration
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "gumiho123"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"

# ğŸ”¹ Load Sentence Transformer Model for Embeddings
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
# ğŸ”¹ Neo4j Connection
driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))


# ğŸ”¹ Function to extract entities from a user query using OpenAI
def extract_entities_from_query(query):
    system_prompt = """
Báº¡n lÃ  chuyÃªn gia trong lÄ©nh vá»±c lá»‹ch sá»­. XÃ¡c Ä‘á»‹nh cÃ¡c thá»±c thá»ƒ lá»‹ch sá»­ tá»« cÃ¢u há»i vÃ  cÃ¡c Ä‘Ã¡p Ã¡n trong truy váº¥n Ä‘Ã£ cho. Danh sÃ¡ch cÃ¡c thá»±c thá»ƒ cÃ³ thá»ƒ náº±m trong cÃ¢u há»i hoáº·c tá»« cÃ¡c Ä‘Ã¡p Ã¡n Ä‘Æ°á»£c cung cáº¥p.

Äá»‹nh dáº¡ng pháº£n há»“i nhÆ° sau:
    ["Entity1", "Entity2", "Entity3"]

CÃ¢u há»i cáº§n trÃ­ch xuáº¥t:
{question}
    """

    response = call_openai(system_prompt.format(question=query))
    print(response)
    return ast.literal_eval(response) # Convert string JSON to Python dict

# ğŸ”¹ Function to get query embedding
def get_query_embedding(query):
    return embedding_model.encode(query).tolist()


# ğŸ”¹ Function to retrieve relevant nodes & relationships from Neo4j based on extracted entities
def retrieve_graph_context(entities, top_k=5):
    retrieved_nodes = []
    related_info = []

    for entity in entities:
        # Retrieve entity descriptions
        cypher_query = """
        MATCH (e:Entity)
        WHERE e.name CONTAINS $entity_name
        RETURN e.name AS name, e.description AS description, gds.similarity.cosine(e.embedding, $query_embedding) AS similarity
        ORDER BY similarity DESC
        LIMIT $top_k
        """

        entity_embedding = get_query_embedding(entity)
        with driver.session() as session:
            results = session.run(cypher_query, entity_name=entity, query_embedding=entity_embedding, top_k=top_k)
            retrieved_nodes.extend([{"name": row["name"], "description": row["description"]} for row in results])

        # Retrieve relationships of the found entities
        cypher_rel_query = """
        MATCH (a:Entity {name: $entity_name})-[r:RELATED_TO]->(b:Entity)
        RETURN a.name AS source, b.name AS target, r.description AS relation_desc
        """
        with driver.session() as session:
            results = session.run(cypher_rel_query, entity_name=entity)
            for row in results:
                related_info.append({
                    "source": row["source"],
                    "target": row["target"],
                    "relation_desc": row["relation_desc"]
                })

    return retrieved_nodes, related_info


# ğŸ”¹ Function to format retrieved graph data as context
def format_context(retrieved_nodes, relationships):
    context = "DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ bá»‘i cáº£nh lá»‹ch sá»­ cÃ³ liÃªn quan:\n\n"

    for node in retrieved_nodes:
        context += f"- {node['name']}: {node['description']}\n"

    if relationships:
        context += "\nRelationships:\n"
        for rel in relationships:
            context += f"- {rel['source']} â†’ {rel['target']}: {rel['relation_desc']}\n"

    return context


# ğŸ”¹ Function to generate response using Mistral
def generate_response(query, context, model):
    prompt = f"{query}\n\n{context}"

    response = ollama_complete(prompt, model=model)
    return response

def graph_rag(query, model="deepseek-v2:16b"):
    print("ğŸ” Extracting entities from query using OpenAI...")
    try:
        extracted_entities = extract_entities_from_query(query)
    except:
        extracted_entities = []
    if not extracted_entities:
        return "âš ï¸ No relevant entities extracted from the query."

    print(f"ğŸ—‚ Identified entities: {extracted_entities}")

    print("ğŸ” Retrieving relevant graph context from Neo4j...")
    retrieved_nodes, relationships = retrieve_graph_context(extracted_entities)

    if not retrieved_nodes:
        return "âš ï¸ No relevant historical context found in the graph database."

    print("ğŸ“– Formatting context...")
    context = format_context(retrieved_nodes, relationships)

    print("ğŸ¤– Generating response using Mistral...")
    response = generate_response(query, context, model)

    return response


# ğŸ”¹ Example Usage
if __name__ == "__main__":
    user_query= """
Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn gia vá» tráº£ lá»i cÃ¢u há»i tráº¯c nghiá»‡m. HÃ£y Ä‘á»c ká»¹ cÃ¢u há»i sau vÃ  chá»‰ tráº£ lá»i báº±ng má»™t kÃ½ tá»± Ä‘áº¡i diá»‡n cho Ä‘Ã¡p Ã¡n Ä‘Ãºng (A, B, C, D). KhÃ´ng giáº£i thÃ­ch, khÃ´ng Ä‘Æ°a thÃªm thÃ´ng tin, chá»‰ tráº£ vá» má»™t kÃ½ tá»± duy nháº¥t:
CÃ¢u há»i:

Thá»ƒ loáº¡i vÄƒn há»c nÃ o cá»§a Trung Quá»‘c thá»i cá»• - trung Ä‘áº¡i Ä‘Ã£ Ä‘áº¡t Ä‘áº¿n Ä‘á»‰nh cao cá»§a nghá»‡ thuáº­t?
A. Sá»­ thi. 
B. ThÆ¡ ÄÆ°á»ng. 
C. Truyá»‡n ngáº¯n. 
D. Ká»‹ch.

Äá»‹nh dáº¡ng Ä‘áº§u ra:
- Chá»‰ tráº£ vá» má»™t kÃ½ tá»± (A, B, C hoáº·c D)

    """
    response = graph_rag(user_query)
    print("\nğŸ’¡ AI Response:\n", response)

# Close Neo4j Connection
driver.close()
